{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.2.2 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers==4.41.2\n",
        "!pip install peft==0.11.1\n",
        "!pip install bitsandbytes==0.43.1  # Consider upgrading to 0.45.0+ for improvements\n",
        "!pip install datasets==2.19.1\n",
        "!pip install accelerate==0.27.2\n",
        "!pip install faiss-cpu\n",
        "!pip uninstall numpy -y\n",
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikknjNf8oc_2",
        "outputId": "030c75c7-73ba-4e7e-9d5f-6ff40041a364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.2.2\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.2.2%2Bcu118-cp311-cp311-linux_x86_64.whl (819.2 MB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.2.2)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.2.2)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.2.2)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "Collecting nvidia-cudnn-cu11==8.7.0.84 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.19.3 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.19.3-py3-none-manylinux1_x86_64.whl (135.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0 (from torch==2.2.2)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.19.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.19.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.18.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.18.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m129.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.5.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.5.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.4.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.4.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.3.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.3.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.2%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.19.3 nvidia-nvtx-cu11-11.8.86 torch-2.2.2+cu118 torchaudio-2.2.2+cu118 torchvision-0.17.2+cu118 triton-2.2.0\n",
            "Collecting transformers==4.41.2\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.2)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2) (2025.4.26)\n",
            "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.2\n",
            "    Uninstalling transformers-4.52.2:\n",
            "      Successfully uninstalled transformers-4.52.2\n",
            "Successfully installed tokenizers-0.19.1 transformers-4.41.2\n",
            "Collecting peft==0.11.1\n",
            "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1) (2.2.2+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1) (0.27.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.11.1) (0.31.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (8.7.0.84)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.8.86)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.11.1) (2.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.11.1) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers->peft==0.11.1) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.11.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.13.0->peft==0.11.1) (1.3.0)\n",
            "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.15.2\n",
            "    Uninstalling peft-0.15.2:\n",
            "      Successfully uninstalled peft-0.15.2\n",
            "Successfully installed peft-0.11.1\n",
            "Collecting bitsandbytes==0.43.1\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.43.1) (2.2.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bitsandbytes==0.43.1) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (8.7.0.84)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (11.8.86)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->bitsandbytes==0.43.1) (2.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->bitsandbytes==0.43.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->bitsandbytes==0.43.1) (1.3.0)\n",
            "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.1\n",
            "Collecting datasets==2.19.1\n",
            "  Using cached datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.19.1)\n",
            "  Using cached pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (0.70.15)\n",
            "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.1)\n",
            "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (0.31.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.2->datasets==2.19.1) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.19.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.19.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.19.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.19.1) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.19.1) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.1) (1.17.0)\n",
            "Using cached datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: pyarrow-hotfix, fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.19.1 fsspec-2024.3.1 pyarrow-hotfix-0.7\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement accelerate==0.28.1 (from versions: 0.0.1, 0.1.0, 0.2.0, 0.2.1, 0.3.0, 0.4.0, 0.5.0, 0.5.1, 0.6.0, 0.6.1, 0.6.2, 0.7.0, 0.7.1, 0.8.0, 0.9.0, 0.10.0, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.13.2, 0.14.0, 0.15.0, 0.16.0, 0.17.0, 0.17.1, 0.18.0, 0.19.0, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0, 0.22.0, 0.23.0, 0.24.0, 0.24.1, 0.25.0, 0.26.0, 0.26.1, 0.27.0, 0.27.1, 0.27.2, 0.28.0, 0.29.0, 0.29.1, 0.29.2, 0.29.3, 0.30.0rc0, 0.30.0, 0.30.1, 0.31.0, 0.32.0, 0.32.1, 0.33.0, 0.34.0, 0.34.1, 0.34.2, 1.0.0rc0, 1.0.0rc1, 1.0.0, 1.0.1, 1.1.0, 1.1.1, 1.2.0, 1.2.1, 1.3.0, 1.4.0, 1.5.0, 1.5.1, 1.5.2, 1.6.0, 1.7.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for accelerate==0.28.1\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "nnItaHvPah94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "268f6d48-d62c-43a2-e789-887a538505af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls /content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/       # adjust if your path is different"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyZzy16Zn8mO",
        "outputId": "b23950d1-19ef-4fd0-8b87-98e8d796c26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dmyGaF0P6jkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLocQEh7iWvz",
        "outputId": "2e239645-08d7-463d-ef8c-6a1b5b551cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip -k *.gz"
      ],
      "metadata": {
        "id": "WL4AXePbiYmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cytigr2ieuP",
        "outputId": "51575d7a-5387-4409-a149-cee140612c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 675M\n",
            "-rw------- 1 root root  17M Feb 22  2021 lbl.json\n",
            "-rw------- 1 root root 3.3M Feb 22  2021 lbl.json.gz\n",
            "-rw------- 1 root root 888K May 29 19:06 test_labels.txt\n",
            "-rw------- 1 root root 938K May 29 19:06 train_labels.txt\n",
            "-rw------- 1 root root 399M Feb 22  2021 trn.json\n",
            "-rw------- 1 root root 184M Feb 22  2021 tst.json\n",
            "-rw------- 1 root root  73M Feb 22  2021 tst.json.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vDJA9MGZicWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf ./data/LF-AmazonTitles-131K.tar.gz -C ./data/"
      ],
      "metadata": {
        "id": "BBoQHwotn--8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l ./data/LF-AmazonTitles-131K/"
      ],
      "metadata": {
        "id": "YjTXwKHcoASA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def convert_json_to_txt(json_file_path, text_output_path, label_output_path):\n",
        "    \"\"\"\n",
        "    Reads a JSON-lines file and splits it into two .txt files:\n",
        "    1. A file with the text content (from the 'title' field).\n",
        "    2. A file with space-separated labels (from the 'target_ind' field).\n",
        "    \"\"\"\n",
        "    print(f\"Processing {json_file_path}...\")\n",
        "    if not os.path.exists(json_file_path):\n",
        "        print(f\"Error: File not found -> {json_file_path}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with open(json_file_path, 'r', encoding='utf-8') as f_json, \\\n",
        "             open(text_output_path, 'w', encoding='utf-8') as f_text, \\\n",
        "             open(label_output_path, 'w', encoding='utf-8') as f_label:\n",
        "\n",
        "            for line in f_json:\n",
        "                # Each line is a JSON object\n",
        "                data = json.loads(line)\n",
        "\n",
        "                # 1. Get the title (or content if you prefer)\n",
        "                # We add a newline character for the .txt file\n",
        "                title = data.get(\"title\", \"\")\n",
        "                f_text.write(title + '\\n')\n",
        "\n",
        "                # 2. Get the labels, convert them to strings, and join with spaces\n",
        "                labels = data.get(\"target_ind\", [])\n",
        "                label_str = ' '.join(map(str, labels))\n",
        "                f_label.write(label_str + '\\n')\n",
        "\n",
        "        print(f\"Successfully created: {text_output_path}\")\n",
        "        print(f\"Successfully created: {label_output_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# --- Main execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming the script is in the same directory as trn.json, tst.json\n",
        "    # Convert training data\n",
        "    convert_json_to_txt(\n",
        "        json_file_path='/content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/tst.json',\n",
        "        text_output_path='/content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/train_texts.txt',\n",
        "        label_output_path='/content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/train_labels.txt'\n",
        "    )\n",
        "\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    # Convert test data\n",
        "    convert_json_to_txt(\n",
        "        json_file_path='/content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/tst.json',\n",
        "        text_output_path='/content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/test_texts.txt',\n",
        "        label_output_path='/content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/test_labels.txt'\n",
        "    )\n",
        "\n",
        "    print(\"\\nConversion complete! Your dataset is now ready for the main script.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b1hVkv-kaAQ",
        "outputId": "e137c280-e1f0-4436-c150-12713bdbf607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/tst.json...\n",
            "Successfully created: /content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/train_texts.txt\n",
            "Successfully created: /content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/train_labels.txt\n",
            "--------------------\n",
            "Processing /content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/tst.json...\n",
            "Successfully created: /content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/test_texts.txt\n",
            "Successfully created: /content/drive/MyDrive/LF-Amazon-131K.raw/LF-Amazon-131K/test_labels.txt\n",
            "\n",
            "Conversion complete! Your dataset is now ready for the main script.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA + 8‑bit Fine‑Tuning for Extreme Multi‑Label Retrieval (Redesigned for Robustness)\n",
        "# Author: Shivaram Kumar (Systems Researcher)\n",
        "# Original Time‑frame: Jan – Feb 2025\n",
        "# Redesign: For error handling and Colab-friendliness\n",
        "# Hardware target: single RTX 3060 Max‑Q (original), Google Colab T4 GPU\n",
        "#\n",
        "# -----------------------------------------------------------------------------\n",
        "# This script covers the full reproducible pipeline:\n",
        "#   • parameter‑efficient fine‑tuning of an MPNet retriever with LoRA (r=8)\n",
        "#   • 8‑bit Adam optimiser via bitsandbytes\n",
        "#   • recall@100 evaluation for extreme multi‑label retrieval (Amazon‑670K‑like)\n",
        "#   • timing / throughput profiler\n",
        "#   • automatic logging of cost–speed trade‑offs\n",
        "#\n",
        "# The code is organised as *one* Python file that can be executed in two modes:\n",
        "#   1.  TRAIN   – fine‑tune and save the LoRA‑adapted 8‑bit model + tokenizer\n",
        "#   2.  BENCH   – measure GPU memory, wall‑clock throughput and recall@100\n",
        "#\n",
        "# Example usage (after placing dataset in data/Amazon670K):\n",
        "#   $ python lora_8bit_retrieval_robust.py train \\\n",
        "#       --dataset_path data/Amazon670K \\\n",
        "#       --output_dir exp/mpnet_lora8_8bit_robust\n",
        "#   $ python lora_8bit_retrieval_robust.py bench \\\n",
        "#       --dataset_path data/Amazon670K \\\n",
        "#       --model_dir exp/mpnet_lora8_8bit_robust\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "#\n",
        "# === FOR GOOGLE COLAB: Run this cell first ===\n",
        "#\n",
        "# !pip install torch==2.2.* torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install transformers==4.41.*\n",
        "# !pip install peft==0.11.*\n",
        "# !pip install bitsandbytes==0.43.*\n",
        "# !pip install datasets==2.19.*\n",
        "# !pip install faiss-cpu==1.8.* # or faiss-gpu if you prefer and have VRAM\n",
        "# !pip install accelerate==0.28.*\n",
        "# !pip install rich>=13\n",
        "# !pip install sentence-transformers # For all-mpnet-base-v2, though AutoModel handles it\n",
        "#\n",
        "# ========================================\n",
        "#\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse, os, sys, json, time, math, shutil, warnings\n",
        "from contextlib import nullcontext\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "import numpy as np # Added for type hinting\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Attempt to import, with guidance if missing\n",
        "try:\n",
        "    from datasets import load_dataset, DatasetDict # Not used in current AmazonXMC, but good for future\n",
        "except ImportError:\n",
        "    print(\"Hugging Face 'datasets' library not found. Please install it if you plan to use it.\")\n",
        "\n",
        "try:\n",
        "    from transformers import (\n",
        "        AutoTokenizer,\n",
        "        AutoModel,\n",
        "        TrainingArguments,\n",
        "        Trainer,\n",
        "        DataCollatorWithPadding,\n",
        "    )\n",
        "    from transformers.trainer_utils import get_last_checkpoint\n",
        "except ImportError:\n",
        "    sys.exit(\"Hugging Face 'transformers' library not found. Please install it: pip install transformers\")\n",
        "\n",
        "try:\n",
        "    from peft import LoraConfig, get_peft_model, TaskType\n",
        "except ImportError:\n",
        "    sys.exit(\"Hugging Face 'peft' library not found. Please install it: pip install peft\")\n",
        "\n",
        "try:\n",
        "    from bitsandbytes.optim import Adam8bit\n",
        "except ImportError:\n",
        "    sys.exit(\"'bitsandbytes' library not found. Please install it: pip install bitsandbytes\")\n",
        "\n",
        "try:\n",
        "    import faiss\n",
        "except ImportError:\n",
        "    sys.exit(\"'faiss' library not found. Please install it: pip install faiss-cpu or faiss-gpu\")\n",
        "\n",
        "try:\n",
        "    from rich.console import Console\n",
        "    from rich.table import Table\n",
        "    from rich.panel import Panel\n",
        "    from rich.text import Text\n",
        "except ImportError:\n",
        "    # Fallback if rich is not installed\n",
        "    class Console:\n",
        "        def print(self, *args, **kwargs):\n",
        "            print(*args)\n",
        "    class Table:\n",
        "        def __init__(self, *args, **kwargs): pass\n",
        "        def add_column(self, *args, **kwargs): pass\n",
        "        def add_row(self, *args, **kwargs): pass\n",
        "    Panel = print\n",
        "    Text = str\n",
        "\n",
        "\n",
        "console = Console()\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset Integrity and Setup\n",
        "# -----------------------------\n",
        "def ensure_dataset_integrity(dataset_root: str, required_files: List[str]) -> bool:\n",
        "    \"\"\"Checks if all required dataset files exist at the specified root.\"\"\"\n",
        "    if not os.path.isdir(dataset_root):\n",
        "        console.print(f\"[bold red]Error: Dataset directory not found: {dataset_root}[/]\")\n",
        "        return False\n",
        "\n",
        "    missing_files = []\n",
        "    for fname in required_files:\n",
        "        fpath = os.path.join(dataset_root, fname)\n",
        "        if not os.path.isfile(fpath):\n",
        "            missing_files.append(fname)\n",
        "\n",
        "    if missing_files:\n",
        "        console.print(f\"[bold red]Error: Missing dataset files in {dataset_root}:[/]\")\n",
        "        for mf in missing_files:\n",
        "            console.print(f\"  - {mf}\")\n",
        "        console.print(\"\\n[bold yellow]Please ensure your dataset (e.g., Amazon-670K) is correctly formatted and placed.\")\n",
        "        console.print(\"The expected structure is:\")\n",
        "        console.print(f\"  {dataset_root}/\")\n",
        "        console.print(f\"    train_texts.txt\")\n",
        "        console.print(f\"    train_labels.txt\")\n",
        "        console.print(f\"    test_texts.txt\")\n",
        "        console.print(f\"    test_labels.txt\")\n",
        "        console.print(\"\\n[bold cyan]For Colab, you might need to:\")\n",
        "        console.print(\"  1. Upload your dataset (e.g., as a zip file) and unzip it.\")\n",
        "        console.print(\"     Example: Create a 'data/Amazon670K' folder in your Colab environment.\")\n",
        "        console.print(\"  2. Or, mount your Google Drive if the dataset is stored there:\")\n",
        "        console.print(\"     from google.colab import drive; drive.mount('/content/drive')\")\n",
        "        console.print(\"     Then adjust --dataset_path to, e.g., '/content/drive/MyDrive/data/Amazon670K'\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset helpers\n",
        "# -----------------------------\n",
        "class AmazonXMC(Dataset):\n",
        "    \"\"\"Thin wrapper exposing (text, labels) for Amazon‑670K‑style files.\n",
        "\n",
        "    Expected directory layout (download links in README of original project):\n",
        "      data/Amazon670K/\n",
        "          train_texts.txt   – one product title per line\n",
        "          train_labels.txt  – space‑separated integer labels per line\n",
        "          test_texts.txt\n",
        "          test_labels.txt\n",
        "\n",
        "    Loading only the first *k* labels per sample is supported through\n",
        "    `topk_labels` to keep memory bounded for quick experiments.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        split: str = \"train\",\n",
        "        topk_labels: int | None = None,\n",
        "        max_samples: int | None = None,\n",
        "    ) -> None:\n",
        "        assert split in {\"train\", \"test\"}, \"Split must be 'train' or 'test'\"\n",
        "        self.text_fp = os.path.join(root, f\"{split}_texts.txt\")\n",
        "        self.label_fp = os.path.join(root, f\"{split}_labels.txt\")\n",
        "        self.texts: List[str] = []\n",
        "        self.labels: List[List[int]] = []\n",
        "\n",
        "        try:\n",
        "            with open(self.text_fp, \"r\", encoding=\"utf‑8\") as ft:\n",
        "                self.texts = [l.rstrip(\"\\n\") for l in ft]\n",
        "            with open(self.label_fp, \"r\", encoding=\"utf‑8\") as fl:\n",
        "                self.labels = [\n",
        "                    [int(t) for t in l.rstrip(\"\\n\").split()[:topk_labels]]\n",
        "                    for l in fl\n",
        "                ]\n",
        "        except FileNotFoundError as e:\n",
        "            console.print(f\"[bold red]Error: Dataset file not found: {e.filename}[/]\")\n",
        "            console.print(f\"Please check the path: {root} and ensure '{split}_texts.txt' and '{split}_labels.txt' exist.\")\n",
        "            raise  # Re-raise the exception to stop execution if files are critical\n",
        "        except Exception as e:\n",
        "            console.print(f\"[bold red]Error loading dataset files from {root}: {e}[/]\")\n",
        "            raise\n",
        "\n",
        "        if len(self.texts) != len(self.labels):\n",
        "            console.print(f\"[bold red]Error: Number of texts ({len(self.texts)}) does not match number of label sets ({len(self.labels)}) in {split} set.[/]\")\n",
        "            raise ValueError(\"Mismatch in number of text samples and label samples.\")\n",
        "\n",
        "        if max_samples:\n",
        "            if max_samples > len(self.texts):\n",
        "                console.print(f\"[yellow]Warning: max_samples ({max_samples}) is greater than available samples ({len(self.texts)}). Using all available samples.[/]\")\n",
        "            self.texts = self.texts[: max_samples]\n",
        "            self.labels = self.labels[: max_samples]\n",
        "\n",
        "        if not self.texts:\n",
        "            console.print(f\"[bold yellow]Warning: Loaded 0 samples for split '{split}' from {root}. This might lead to errors downstream.[/]\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if not (0 <= idx < len(self.texts)):\n",
        "            raise IndexError(f\"Index {idx} out of bounds for dataset with length {len(self.texts)}\")\n",
        "        return {\"text\": self.texts[idx], \"labels\": self.labels[idx]}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Model + LoRA helpers\n",
        "# -----------------------------\n",
        "\n",
        "def load_backbone(model_name: str, load_in_8bit: bool = True):\n",
        "    console.print(f\"[bold cyan]Loading {model_name} (8‑bit={load_in_8bit})...[/]\")\n",
        "\n",
        "    # MPNet models have compatibility issues with quantization\n",
        "    is_mpnet = \"mpnet\" in model_name.lower()\n",
        "    use_quantization = load_in_8bit and not is_mpnet\n",
        "\n",
        "    if is_mpnet and load_in_8bit:\n",
        "        console.print(\"[yellow]Warning: MPNet models have issues with 8-bit quantization. Loading with fp16 instead.[/]\")\n",
        "\n",
        "    try:\n",
        "        if use_quantization:\n",
        "            from transformers import BitsAndBytesConfig\n",
        "            quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "            model = AutoModel.from_pretrained(\n",
        "                model_name,\n",
        "                quantization_config=quantization_config,\n",
        "                device_map=\"auto\",  # Safe for non-MPNet models\n",
        "                trust_remote_code=True,\n",
        "            )\n",
        "        else:\n",
        "            # For MPNet: use fp16 instead of quantization\n",
        "            model = AutoModel.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16,  # Half precision for memory efficiency\n",
        "                trust_remote_code=True,\n",
        "            )\n",
        "            # Manually move to GPU for non-quantized models\n",
        "            if torch.cuda.is_available():\n",
        "                model = model.cuda()\n",
        "\n",
        "        #console.print(\"[bold yellow]Inspecting model named_modules():[/]\")\n",
        "        #for name, module in model.named_modules():\n",
        "        #    console.print(f\"  - {name}\")\n",
        "        #console.print(\"[/]\")\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        tokenizer.truncation_side = \"right\"\n",
        "\n",
        "        if tokenizer.pad_token is None:\n",
        "            console.print(\"[yellow]Warning: Tokenizer does not have a PAD token. Setting to EOS token.[/]\")\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        console.print(f\"[green]✓ Model {model_name} loaded successfully (quantized: {use_quantization}).[/]\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        console.print(f\"[bold red]Error loading model or tokenizer '{model_name}': {e}[/]\")\n",
        "        raise\n",
        "\n",
        "\n",
        "\n",
        "def inject_lora(model, r: int = 8, alpha: int = 16, dropout: float = 0.1):\n",
        "    \"\"\"Wrap key projection modules with LoRA adapters.\"\"\"\n",
        "    # Target modules depend on the model architecture. For MPNet:\n",
        "    #target_modules = [\"q_proj\", \"v_proj\"] # Correct for MPNet-like models from sentence-transformers\n",
        "    target_modules = [\"q\", \"v\"]\n",
        "    # For other BERT-like models, it might be [\"query\", \"value\"]\n",
        "    # It's good to inspect model.named_modules() to confirm target_modules for a new architecture.\n",
        "\n",
        "    lora_cfg = LoraConfig(\n",
        "        task_type=TaskType.FEATURE_EXTRACTION, # Using this as we are extracting embeddings\n",
        "        r=r,\n",
        "        lora_alpha=alpha,\n",
        "        lora_dropout=dropout,\n",
        "        bias=\"none\", # Recommended for LoRA\n",
        "        target_modules=target_modules,\n",
        "    )\n",
        "    try:\n",
        "        lora_model = get_peft_model(model, lora_cfg)\n",
        "        # Ensure trainable LoRA parameters are float32 when using AMP (fp16=True)\n",
        "        # This is crucial if the base model was loaded in fp16.\n",
        "        for param in lora_model.parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data = param.data.float()\n",
        "        console.print(f\"[green]✓ LoRA adapters (r={r}, alpha={alpha}) injected.[/]\")\n",
        "        lora_model.print_trainable_parameters()\n",
        "        return lora_model\n",
        "    except Exception as e:\n",
        "        console.print(f\"[bold red]Error injecting LoRA adapters: {e}[/]\")\n",
        "        console.print(\"[bold yellow]Potential issues:[/]\")\n",
        "        console.print(f\"  - `target_modules` ({target_modules}) might be incorrect for the loaded model architecture.\")\n",
        "        console.print(\"  - Incompatibility with the 'peft' library version.\")\n",
        "        raise\n",
        "\n",
        "# -----------------------------\n",
        "# Trainer plumbing\n",
        "# -----------------------------\n",
        "\n",
        "def collate_fn(tokenizer, max_length=128): # Made max_length a parameter\n",
        "    # DataCollatorWithPadding handles padding to the longest sequence in the batch\n",
        "    # It's generally preferred over tokenizer(..., padding='max_length') unless all sequences must be padded to max_length\n",
        "    # For dynamic padding:\n",
        "    # data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='longest')\n",
        "    # However, the original code tokenizes manually, which is fine.\n",
        "\n",
        "    def _fn(batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        texts = [b[\"text\"] for b in batch]\n",
        "        labels = [b[\"labels\"] for b in batch] # List of lists of integers\n",
        "\n",
        "        try:\n",
        "            # Ensure max_length is reasonable for the model\n",
        "            # Most sentence transformers like MPNet have a max sequence length of 128-512\n",
        "            # Using a fixed max_length here for consistent tensor shapes if desired,\n",
        "            # but padding='longest' (default for DataCollatorWithPadding) is often more efficient.\n",
        "            tokenised = tokenizer(\n",
        "                texts,\n",
        "                padding=\"longest\", # Pad to longest in batch\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            # The original had `labels` as a list of lists, which is fine for custom eval\n",
        "            # but if Trainer's compute_metrics or loss needed it as tensors, it would need adjustment.\n",
        "            # Here, `compute_loss` handles the embeddings directly, and eval is custom.\n",
        "            tokenised[\"labels\"] = labels\n",
        "            return tokenised\n",
        "        except Exception as e:\n",
        "            console.print(f\"[bold red]Error during tokenization or collation: {e}[/]\")\n",
        "            # Log a snippet of the problematic text if possible and safe\n",
        "            if texts:\n",
        "                console.print(f\"Problematic text sample (first 50 chars): '{texts[0][:50]}...'\")\n",
        "            raise\n",
        "    return _fn\n",
        "\n",
        "\n",
        "def compute_loss(model, batch_inputs, device): # Added device parameter\n",
        "    \"\"\"Contrastive / multi‑label retrieval loss (pairwise NT‑Xent light).\"\"\"\n",
        "    if device is None:\n",
        "      device = next(model.parameters()).device\n",
        "\n",
        "    # Prepare inputs for the model, ensuring they are on the correct device\n",
        "    model_inputs = {k: v.to(device) for k, v in batch_inputs.items() if k != \"labels\" and hasattr(v, 'to')}\n",
        "\n",
        "    try:\n",
        "        outputs = model(**model_inputs)\n",
        "        # CLS token embedding is typically at index 0 of the first token's hidden state\n",
        "        emb = outputs.last_hidden_state[:, 0, :]  # (Batch, HiddenDim)\n",
        "        emb = F.normalize(emb, p=2, dim=-1) # L2 normalization\n",
        "\n",
        "        # Cosine similarity matrix\n",
        "        sim_matrix = emb @ emb.T  # (Batch, Batch)\n",
        "\n",
        "        # NT-Xent Loss: treats each sample as its own class (positive example)\n",
        "        # and all other samples in the batch as negative examples.\n",
        "        # Labels for cross_entropy are just 0, 1, 2, ..., B-1\n",
        "        labels = torch.arange(emb.size(0), device=device)\n",
        "\n",
        "        # Temperature scaling (optional, but common in contrastive learning)\n",
        "        # temperature = 0.07 # Example value\n",
        "        # loss = F.cross_entropy(sim_matrix / temperature, labels)\n",
        "        loss = F.cross_entropy(sim_matrix, labels)\n",
        "        return loss\n",
        "    except Exception as e:\n",
        "        console.print(f\"[bold red]Error during loss computation: {e}[/]\")\n",
        "        # Potentially log shapes of tensors involved if a shape mismatch error occurs\n",
        "        console.print(f\"Input keys: {model_inputs.keys()}\")\n",
        "        if 'last_hidden_state' in outputs:\n",
        "             console.print(f\"Shape of last_hidden_state: {outputs.last_hidden_state.shape}\")\n",
        "        raise\n",
        "\n",
        "class RetrievalTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        device = next(model.parameters()).device\n",
        "        # `inputs` already contains tensors on the correct device from DataLoader\n",
        "        loss = compute_loss(model, inputs, device) # Pass model.device\n",
        "        # `return_outputs=True` expects (loss, outputs_dict)\n",
        "        # Here, outputs_dict is not strictly necessary if we only care about loss for training step\n",
        "        return (loss, {\"loss\": loss}) if return_outputs else loss\n",
        "\n",
        "    # prediction_step is used for evaluation if `compute_metrics` is defined\n",
        "    # or if `predict()` is called.\n",
        "    def prediction_step(\n",
        "        self,\n",
        "        model: torch.nn.Module,\n",
        "        inputs: Dict[str, Any],\n",
        "        prediction_loss_only: bool,\n",
        "        ignore_keys: Optional[List[str]] = None,\n",
        "    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "        device = next(model.parameters()).device\n",
        "        model_inputs = {k: v.to(model.device) for k, v in inputs.items() if k != \"labels\" and hasattr(v, 'to')}\n",
        "        raw_labels = inputs[\"labels\"] # Keep raw labels (list of lists of int)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                outputs = model(**model_inputs)\n",
        "                emb = outputs.last_hidden_state[:, 0, :]\n",
        "                emb = F.normalize(emb, p=2, dim=-1)\n",
        "            except Exception as e:\n",
        "                console.print(f\"[bold red]Error during prediction_step (model inference): {e}[/]\")\n",
        "                # Return None for embeddings if error occurs, Trainer might handle this\n",
        "                return (None, None, None)\n",
        "        placeholder_labels = torch.arange(emb.size(0), device=emb.device)\n",
        "        # No loss computed here by default unless `prediction_loss_only` is False and `compute_loss` is adapted\n",
        "        # For current custom bench, we only need embeddings and labels.\n",
        "        # The trainer expects (optional_loss, logits_or_embeddings, labels_for_metrics)\n",
        "        # We return None for loss, our embeddings, and the raw labels.\n",
        "        #return (None, emb.cpu(), raw_labels)\n",
        "        return (None, emb.cpu(), placeholder_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# recall@K evaluation (Corrected Logic)\n",
        "# -----------------------------\n",
        "\n",
        "def build_faiss_index(embs: np.ndarray, dim: int) -> Optional[faiss.Index]:\n",
        "    try:\n",
        "        # Using IndexFlatIP because embeddings are normalized (IP = Cosine Similarity)\n",
        "        index = faiss.IndexFlatIP(dim)\n",
        "        index.add(embs)\n",
        "        console.print(f\"[green]✓ FAISS index built with {embs.shape[0]} embeddings of dim {dim}.[/]\")\n",
        "        return index\n",
        "    except Exception as e:\n",
        "        console.print(f\"[bold red]Error building FAISS index: {e}[/]\")\n",
        "        console.print(\"[bold yellow]Ensure FAISS is correctly installed and embeddings are valid (numpy array, correct dtype).[/]\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def recall_at_k(\n",
        "    query_embeddings: np.ndarray,\n",
        "    query_true_labels: List[List[int]], # Labels for each query [[1,2], [3], ...]\n",
        "    corpus_item_embeddings: np.ndarray, # Embeddings of all items in the retrieval corpus\n",
        "    corpus_item_true_labels: List[List[int]], # Labels for each item in the corpus\n",
        "    faiss_index: faiss.Index, # Pre-built FAISS index on corpus_item_embeddings\n",
        "    k_val: int = 100,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates recall@k for multi-label retrieval.\n",
        "    A query is considered a \"hit\" if any of its true labels are found\n",
        "    among the true labels of any of the top-k retrieved corpus items.\n",
        "    \"\"\"\n",
        "    if query_embeddings.shape[0] != len(query_true_labels):\n",
        "        console.print(f\"[bold red]Mismatch in query embeddings ({query_embeddings.shape[0]}) and query labels ({len(query_true_labels)}) count.[/]\")\n",
        "        return 0.0\n",
        "    if faiss_index.ntotal != len(corpus_item_true_labels):\n",
        "        console.print(f\"[bold red]Mismatch in FAISS index size ({faiss_index.ntotal}) and corpus labels count ({len(corpus_item_true_labels)}).[/]\")\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        # D: distances (similarity scores), I: indices of retrieved corpus items\n",
        "        D, I = faiss_index.search(query_embeddings, k_val)\n",
        "    except Exception as e:\n",
        "        console.print(f\"[bold red]Error during FAISS search: {e}[/]\")\n",
        "        return 0.0\n",
        "\n",
        "    num_queries = query_embeddings.shape[0]\n",
        "    hits = 0\n",
        "\n",
        "    for i in range(num_queries):\n",
        "        current_query_true_labels_set = set(query_true_labels[i])\n",
        "        if not current_query_true_labels_set: # Skip if query has no true labels\n",
        "            continue\n",
        "\n",
        "        retrieved_corpus_indices = I[i] # Indices of top-k items from corpus\n",
        "\n",
        "        found_match_for_this_query = False\n",
        "        for corpus_idx in retrieved_corpus_indices:\n",
        "            if corpus_idx == -1: # FAISS can return -1 if fewer than k items are found\n",
        "                continue\n",
        "\n",
        "            # Get the true labels of the retrieved corpus item\n",
        "            retrieved_corpus_item_labels_set = set(corpus_item_true_labels[corpus_idx])\n",
        "\n",
        "            # Check for intersection\n",
        "            if not current_query_true_labels_set.isdisjoint(retrieved_corpus_item_labels_set):\n",
        "                found_match_for_this_query = True\n",
        "                break # Move to next query once a match is found\n",
        "\n",
        "        if found_match_for_this_query:\n",
        "            hits += 1\n",
        "\n",
        "    if num_queries == 0:\n",
        "        return 0.0\n",
        "\n",
        "    recall = (hits / num_queries) * 100.0\n",
        "    return recall\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Training routine\n",
        "# -----------------------------\n",
        "\n",
        "def train(args):\n",
        "    console.print(Panel(\"[bold green]Initializing Training Process[/]\", expand=False))\n",
        "\n",
        "\n",
        "     # --- BEGIN ADDED CHECK TO SKIP COMPLETED TRAINING ---\n",
        "    adapter_model_path_safetensors = os.path.join(args.output_dir, \"adapter_model.safetensors\")\n",
        "    adapter_model_path_bin = os.path.join(args.output_dir, \"adapter_model.bin\") # Older format\n",
        "    adapter_config_path = os.path.join(args.output_dir, \"adapter_config.json\")\n",
        "    tokenizer_config_path = os.path.join(args.output_dir, \"tokenizer_config.json\") # Also check tokenizer\n",
        "\n",
        "    final_adapter_exists = os.path.exists(adapter_model_path_safetensors) or os.path.exists(adapter_model_path_bin)\n",
        "\n",
        "    if final_adapter_exists and os.path.exists(adapter_config_path) and os.path.exists(tokenizer_config_path):\n",
        "        console.print(f\"[bold yellow]Final LoRA adapter model and tokenizer already found in {args.output_dir}.[/]\")\n",
        "        console.print(f\"[yellow]Skipping fine-tuning. To re-train, clear or use a different --output_dir.[/]\")\n",
        "\n",
        "        # Optionally, verify if number of epochs matches or if a 'completed.flag' file exists\n",
        "        # For now, existence of key output files is a good heuristic.\n",
        "        return\n",
        "\n",
        "\n",
        "    required_files = [\"train_texts.txt\", \"train_labels.txt\", \"test_texts.txt\", \"test_labels.txt\"]\n",
        "    if not ensure_dataset_integrity(args.dataset_path, required_files):\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # 1. Load backbone + LoRA\n",
        "        model, tokenizer = load_backbone(args.model_name, load_in_8bit=True)\n",
        "        # Make sure PAD token is set if tokenizer doesn't have one (e.g., for Llama if used)\n",
        "        if tokenizer.pad_token_id is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "        model = inject_lora(model, r=args.lora_rank, alpha=args.lora_alpha) # Added lora_alpha\n",
        "\n",
        "        # 2. Prepare dataset\n",
        "        console.print(f\"[cyan]Loading datasets from {args.dataset_path}...[/]\")\n",
        "        train_ds = AmazonXMC(args.dataset_path, \"train\", max_samples=args.train_samples, topk_labels=args.topk_labels_train)\n",
        "        val_ds = AmazonXMC(args.dataset_path, \"test\", max_samples=args.val_samples, topk_labels=args.topk_labels_val)\n",
        "\n",
        "        if len(train_ds) == 0:\n",
        "            console.print(\"[bold red]Training dataset is empty. Aborting training.[/]\")\n",
        "            return\n",
        "        if len(val_ds) == 0:\n",
        "            console.print(\"[bold yellow]Warning: Validation dataset is empty. Evaluation will be skipped or may fail.[/]\")\n",
        "\n",
        "        # The collate_fn will use tokenizer.model_max_length if args.max_seq_length is not set.\n",
        "        # It's good practice to set it explicitly or ensure the tokenizer has a sensible default.\n",
        "        effective_max_length = args.max_seq_length if args.max_seq_length else tokenizer.model_max_length\n",
        "        if not effective_max_length or effective_max_length > 2048: # Safety for very large/missing max_length\n",
        "            console.print(f\"[yellow]Warning: tokenizer.model_max_length is {tokenizer.model_max_length}. Clamping to 512 for collation.[/yellow]\")\n",
        "            effective_max_length = 512\n",
        "\n",
        "\n",
        "        # 3. TrainingArguments w/ 8‑bit Adam\n",
        "        # Ensure output_dir exists\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "        targs = TrainingArguments(\n",
        "            output_dir=args.output_dir,\n",
        "            num_train_epochs=args.epochs,\n",
        "            per_device_train_batch_size=args.batch_size,\n",
        "            per_device_eval_batch_size=args.eval_batch_size,\n",
        "            gradient_accumulation_steps=args.grad_accum,\n",
        "            fp16=True, # Mixed precision training\n",
        "            evaluation_strategy=\"epoch\" if len(val_ds) > 0 else \"no\",\n",
        "            save_strategy=\"epoch\", # Save model at the end of each epoch\n",
        "            logging_steps=args.logging_steps, # Log every N steps\n",
        "            save_total_limit=args.save_total_limit, # Only keep the last N checkpoints\n",
        "            learning_rate=args.lr,\n",
        "            weight_decay=args.weight_decay,\n",
        "            report_to=args.report_to.split(',') if args.report_to else [\"none\"], # e.g. \"tensorboard,wandb\"\n",
        "            dataloader_num_workers=args.num_workers, # For faster data loading\n",
        "            remove_unused_columns=False, # Important because we keep \"labels\"\n",
        "            load_best_model_at_end=False if len(val_ds) > 0 else False, # Load best model based on eval\n",
        "            metric_for_best_model=None if len(val_ds) > 0 else None, # Default to loss if no custom metric\n",
        "            greater_is_better=None if len(val_ds) > 0 else None,\n",
        "        )\n",
        "\n",
        "        # Custom optimizer: Adam8bit\n",
        "        # The Trainer will use model.parameters() by default.\n",
        "        # For LoRA, we only want to optimize trainable parameters.\n",
        "        optimizer = Adam8bit(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)\n",
        "        # lr_scheduler is typically handled by Trainer, but can be passed too. (optimizer, scheduler)\n",
        "\n",
        "        trainer = RetrievalTrainer(\n",
        "            model=model,\n",
        "            args=targs,\n",
        "            train_dataset=train_ds,\n",
        "            eval_dataset=val_ds if len(val_ds) > 0 else None,\n",
        "            data_collator=collate_fn(tokenizer, max_length=effective_max_length),\n",
        "            optimizers=(optimizer, None), # Pass the custom optimizer\n",
        "        )\n",
        "        # If you had custom compute_metrics for evaluation:\n",
        "        # trainer.compute_metrics = compute_metrics_function\n",
        "\n",
        "        # 4. Train\n",
        "        console.print(\"[bold green]Starting fine‑tuning...[/]\")\n",
        "        # Check for last checkpoint\n",
        "        last_checkpoint_dir = get_last_checkpoint(args.output_dir)\n",
        "        valid_checkpoint_to_resume = None\n",
        "        if last_checkpoint_dir:\n",
        "            # Check if the found checkpoint is complete by looking for trainer_state.json\n",
        "            trainer_state_path = os.path.join(last_checkpoint_dir, \"trainer_state.json\")\n",
        "            adapter_model_path_sft = os.path.join(last_checkpoint_dir, \"adapter_model.safetensors\")\n",
        "            adapter_model_path_bin = os.path.join(last_checkpoint_dir, \"adapter_model.bin\")\n",
        "            pytorch_model_path = os.path.join(last_checkpoint_dir, \"pytorch_model.bin\") # For full model checkpoints\n",
        "\n",
        "            # A checkpoint is considered valid if trainer_state.json and some model weights exist\n",
        "            has_trainer_state = os.path.exists(trainer_state_path)\n",
        "            has_adapter_model = os.path.exists(adapter_model_path_sft) or os.path.exists(adapter_model_path_bin)\n",
        "            has_pytorch_model = os.path.exists(pytorch_model_path) # Might be saved by older trainer or non-PEFT\n",
        "\n",
        "            if has_trainer_state and (has_adapter_model or has_pytorch_model):\n",
        "                valid_checkpoint_to_resume = last_checkpoint_dir\n",
        "                console.print(f\"[yellow]Resuming training from complete checkpoint: {valid_checkpoint_to_resume}[/yellow]\")\n",
        "            else:\n",
        "                console.print(f\"[bold orange_red1]Warning: Last checkpoint directory '{last_checkpoint_dir}' found but appears incomplete (missing trainer_state.json or model files).[/]\")\n",
        "                # Try to find the second to last checkpoint if save_total_limit allows\n",
        "                # This is more complex, for now, we'll just warn and potentially start fresh or let user handle.\n",
        "                # A simple approach is to just not resume if the last one is broken.\n",
        "                # For a more robust solution, you might list all checkpoint-* dirs, sort them,\n",
        "                # and iterate backwards checking for completeness.\n",
        "                console.print(f\"[cyan]Attempting to find an older complete checkpoint or starting fresh.[/]\")\n",
        "                # As a basic recovery, let's try to see if Trainer can handle a potentially problematic path\n",
        "                # or if it's better to pass None and force fresh start if last is broken.\n",
        "                # For now, if it's incomplete, we won't resume from it with this explicit check.\n",
        "                # last_checkpoint_dir = None # Effectively start fresh if most recent is broken.\n",
        "                # OR, you could implement logic to find the *next oldest* valid one.\n",
        "                # For this fix, let's just inform the user and proceed with `None` if incomplete.\n",
        "                if not has_trainer_state:\n",
        "                    console.print(f\"  - Missing: {trainer_state_path}\")\n",
        "                if not (has_adapter_model or has_pytorch_model):\n",
        "                    console.print(f\"  - Missing: Model files (e.g., adapter_model.safetensors or pytorch_model.bin)\")\n",
        "\n",
        "                # Attempt to find the next valid checkpoint by looking at parent directory's checkpoints\n",
        "                # This is a simplified version. Transformers has internal logic too.\n",
        "                all_checkpoints = [os.path.join(args.output_dir, d) for d in os.listdir(args.output_dir) if d.startswith(\"checkpoint-\")]\n",
        "                all_checkpoints.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
        "\n",
        "                for chkpt_path in reversed(all_checkpoints):\n",
        "                    if chkpt_path == last_checkpoint_dir: # Skip the one we know is bad\n",
        "                        continue\n",
        "                    ts_path = os.path.join(chkpt_path, \"trainer_state.json\")\n",
        "                    am_path_sft = os.path.join(chkpt_path, \"adapter_model.safetensors\")\n",
        "                    am_path_bin = os.path.join(chkpt_path, \"adapter_model.bin\")\n",
        "                    pm_path = os.path.join(chkpt_path, \"pytorch_model.bin\")\n",
        "                    if os.path.exists(ts_path) and (os.path.exists(am_path_sft) or os.path.exists(am_path_bin) or os.path.exists(pm_path)):\n",
        "                        valid_checkpoint_to_resume = chkpt_path\n",
        "                        console.print(f\"[yellow]Found older complete checkpoint to resume from: {valid_checkpoint_to_resume}[/yellow]\")\n",
        "                        break\n",
        "                if not valid_checkpoint_to_resume:\n",
        "                     console.print(f\"[cyan]No complete checkpoint found in {args.output_dir}. Starting fresh training.[/]\")\n",
        "        else:\n",
        "            console.print(f\"[cyan]No checkpoint found in {args.output_dir}. Starting fresh training.[/]\")\n",
        "\n",
        "        train_result = trainer.train(resume_from_checkpoint=last_checkpoint_dir)\n",
        "        console.print(Panel(f\"[bold green]✓ Training finished. Results: {train_result.metrics}[/]\", expand=False))\n",
        "\n",
        "        # 5. Save final model, tokenizer, and training state\n",
        "        trainer.save_model(args.output_dir) # Saves LoRA adapter and config\n",
        "        # To save the full model (if needed, though LoRA is usually separate):\n",
        "        # model.save_pretrained(os.path.join(args.output_dir, \"full_model\"))\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "        trainer.save_state() # Saves optimizer state, RNG state, etc.\n",
        "\n",
        "        console.print(f\"[bold green]✓ Model, tokenizer, and training state saved to {args.output_dir}[/]\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        console.print(f\"[bold red]Training Error: A required file was not found: {e}[/]\")\n",
        "    except ValueError as e:\n",
        "        console.print(f\"[bold red]Training Error: A value was incorrect: {e}[/]\")\n",
        "    except RuntimeError as e: # Catch CUDA errors, etc.\n",
        "        console.print(f\"[bold red]Training Runtime Error: {e}[/]\")\n",
        "        console.print(\"[bold yellow]This could be a CUDA OOM error. Try reducing batch size or sequence length.[/]\")\n",
        "    except Exception as e:\n",
        "        console.print(f\"[bold red]An unexpected error occurred during training: {e}[/]\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Benchmark routine\n",
        "# -----------------------------\n",
        "def bench(args):\n",
        "    device = next(model.parameters()).device\n",
        "    console.print(Panel(\"[bold blue]Initializing Benchmarking Process[/]\", expand=False))\n",
        "\n",
        "    # Ensure model directory exists\n",
        "    if not os.path.isdir(args.model_dir):\n",
        "        console.print(f\"[bold red]Error: Model directory not found: {args.model_dir}[/]\")\n",
        "        console.print(\"Please ensure you have a trained model saved (e.g., after running the 'train' command).\")\n",
        "        return\n",
        "\n",
        "    required_files = [\"train_texts.txt\", \"train_labels.txt\", \"test_texts.txt\", \"test_labels.txt\"] # Corpus needs train files\n",
        "    if not ensure_dataset_integrity(args.dataset_path, required_files):\n",
        "        return\n",
        "\n",
        "    # In the bench function, replace the model loading section with:\n",
        "    if load_in_8bit:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "        model = AutoModel.from_pretrained(\n",
        "            base_model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            #device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "    else:\n",
        "        model = AutoModel.from_pretrained(\n",
        "            base_model_name,\n",
        "            #device_map=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "\n",
        "    # Then apply LoRA adapters\n",
        "    model = get_peft_model(model, LoraConfig.from_pretrained(args.model_dir))\n",
        "\n",
        "\n",
        "    try:\n",
        "        # 1. Load fine-tuned LoRA model (PEFT will handle loading adapters)\n",
        "        # When loading a PEFT model, you typically load the base model first, then apply adapters.\n",
        "        # However, if `save_pretrained` was used on a PeftModel, it saves adapter config.\n",
        "        # Let's assume the `model_dir` contains the LoRA adapter and the original base model name is known or saved.\n",
        "        # For simplicity, we reload the base model and then apply LoRA weights from `model_dir`.\n",
        "\n",
        "        # First, determine the base model name (this might be saved in adapter_config.json or known)\n",
        "        # For this script, we assume the same base model as training.\n",
        "        # A more robust way would be to save base_model_name_or_path in PeftConfig.\n",
        "        base_model_name = args.base_model_name_for_bench # Add this as an arg or infer\n",
        "\n",
        "        model, tokenizer = load_backbone(base_model_name, load_in_8bit=True)\n",
        "        model = get_peft_model(model, LoraConfig.from_pretrained(args.model_dir)) # Load LoRA adapters\n",
        "\n",
        "        # Alternatively, if you saved the full model with adapters merged:\n",
        "        # model = AutoModel.from_pretrained(args.model_dir, load_in_8bit=True, device_map=\"auto\")\n",
        "        # tokenizer = AutoTokenizer.from_pretrained(args.model_dir)\n",
        "\n",
        "        model.eval() # Set model to evaluation mode\n",
        "        console.print(f\"[green]✓ Fine-tuned LoRA model and tokenizer loaded from {args.model_dir}.[/]\")\n",
        "\n",
        "        effective_max_length = args.max_seq_length if args.max_seq_length else tokenizer.model_max_length\n",
        "        if not effective_max_length or effective_max_length > 2048:\n",
        "            effective_max_length = 512\n",
        "\n",
        "\n",
        "        # 2. Prepare datasets for corpus and queries\n",
        "        console.print(\"[cyan]Loading corpus (from training set) and query (from test set) data...[/]\")\n",
        "        # Corpus for FAISS index (typically from training data)\n",
        "        corpus_dataset = AmazonXMC(args.dataset_path, \"train\", max_samples=args.index_samples, topk_labels=args.topk_labels_val) # Use val topk for consistency\n",
        "        # Query dataset (typically from test data)\n",
        "        query_dataset = AmazonXMC(args.dataset_path, \"test\", max_samples=args.val_samples, topk_labels=args.topk_labels_val)\n",
        "\n",
        "        if len(corpus_dataset) == 0 or len(query_dataset) == 0:\n",
        "            console.print(\"[bold red]Corpus or query dataset is empty. Aborting benchmark.[/]\")\n",
        "            return\n",
        "\n",
        "        corpus_loader = DataLoader(corpus_dataset, batch_size=args.eval_batch_size, shuffle=False, collate_fn=collate_fn(tokenizer, max_length=effective_max_length))\n",
        "        query_loader = DataLoader(query_dataset, batch_size=args.eval_batch_size, shuffle=False, collate_fn=collate_fn(tokenizer, max_length=effective_max_length))\n",
        "\n",
        "        dim = model.config.hidden_size # Embedding dimension\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "        # Helper to encode datasets\n",
        "        def encode_dataset(loader: DataLoader, model_to_encode_with: torch.nn.Module) -> np.ndarray:\n",
        "            all_embeddings = []\n",
        "            model_to_encode_with.eval() # Ensure model is in eval mode\n",
        "            with torch.no_grad():\n",
        "                for batch in loader:\n",
        "                    model_inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\" and hasattr(v, 'to')}\n",
        "                    outputs = model_to_encode_with(**model_inputs)\n",
        "                    emb = outputs.last_hidden_state[:, 0, :]\n",
        "                    emb = F.normalize(emb, p=2, dim=-1)\n",
        "                    all_embeddings.append(emb.cpu().numpy())\n",
        "            return np.concatenate(all_embeddings, axis=0)\n",
        "\n",
        "        # 3. Encode corpus and build FAISS index\n",
        "        console.print(\"[cyan]Encoding corpus items...[/]\")\n",
        "        t_start_corpus_encode = time.perf_counter()\n",
        "        corpus_embeddings_np = encode_dataset(corpus_loader, model)\n",
        "        corpus_encode_time = time.perf_counter() - t_start_corpus_encode\n",
        "        console.print(f\"  Corpus encoding took: {corpus_encode_time:.2f}s for {len(corpus_embeddings_np)} items.\")\n",
        "\n",
        "        t_start_faiss_build = time.perf_counter()\n",
        "        faiss_index = build_faiss_index(corpus_embeddings_np, dim)\n",
        "        if faiss_index is None: return # Error handled in build_faiss_index\n",
        "        faiss_build_time = time.perf_counter() - t_start_faiss_build\n",
        "        console.print(f\"  FAISS index build took: {faiss_build_time:.2f}s.\")\n",
        "\n",
        "        build_time_total = corpus_encode_time + faiss_build_time\n",
        "\n",
        "        # 4. Encode queries and perform search\n",
        "        console.print(\"[cyan]Encoding query items...[/]\")\n",
        "        t_start_query_encode = time.perf_counter()\n",
        "        query_embeddings_np = encode_dataset(query_loader, model)\n",
        "        query_encode_time = time.perf_counter() - t_start_query_encode\n",
        "        console.print(f\"  Query encoding took: {query_encode_time:.2f}s for {len(query_embeddings_np)} items.\")\n",
        "\n",
        "        # Prepare labels for recall calculation\n",
        "        corpus_true_labels_list = corpus_dataset.labels # List of lists from AmazonXMC\n",
        "        query_true_labels_list = query_dataset.labels   # List of lists from AmazonXMC\n",
        "\n",
        "        console.print(f\"[cyan]Performing search and calculating recall@{args.recall_k_val}...[/]\")\n",
        "        t_start_search = time.perf_counter()\n",
        "        recall = recall_at_k(\n",
        "            query_embeddings_np,\n",
        "            query_true_labels_list,\n",
        "            corpus_embeddings_np, # Pass corpus embeddings (not strictly needed if index is passed)\n",
        "            corpus_true_labels_list,\n",
        "            faiss_index,\n",
        "            k_val=args.recall_k_val\n",
        "        )\n",
        "        search_time = time.perf_counter() - t_start_search\n",
        "        console.print(f\"  FAISS search and recall calculation took: {search_time:.2f}s.\")\n",
        "\n",
        "        # Throughput calculation\n",
        "        total_query_processing_time = query_encode_time + search_time\n",
        "        qps = len(query_dataset) / total_query_processing_time if total_query_processing_time > 0 else 0\n",
        "\n",
        "        # 5. Log results\n",
        "        # Ensure model_dir for saving results exists (it should if model was loaded)\n",
        "        os.makedirs(args.model_dir, exist_ok=True)\n",
        "        summary_save_path = os.path.join(args.model_dir, f\"bench_summary_k{args.recall_k_val}.json\")\n",
        "\n",
        "        # GPU Memory\n",
        "        peak_mem_gb = 0.0\n",
        "        if torch.cuda.is_available():\n",
        "            peak_mem_gb = torch.cuda.max_memory_allocated() / 1e9\n",
        "            torch.cuda.reset_peak_memory_stats() # Reset for next potential measurement\n",
        "\n",
        "        summary = {\n",
        "            \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
        "            \"model_dir\": args.model_dir,\n",
        "            \"base_model_name\": base_model_name,\n",
        "            \"dataset_path\": args.dataset_path,\n",
        "            \"index_samples\": len(corpus_dataset),\n",
        "            \"query_samples\": len(query_dataset),\n",
        "            f\"recall@{args.recall_k_val}\": recall,\n",
        "            \"throughput_qps (queries_encoded_and_searched/sec)\": qps,\n",
        "            \"corpus_encode_time_s\": corpus_encode_time,\n",
        "            \"faiss_index_build_time_s\": faiss_build_time,\n",
        "            \"total_index_build_time_s\": build_time_total,\n",
        "            \"query_encode_time_s\": query_encode_time,\n",
        "            \"search_and_recall_calc_time_s\": search_time,\n",
        "            \"gpu_peak_mem_gb\": peak_mem_gb,\n",
        "        }\n",
        "\n",
        "        with open(summary_save_path, \"w\") as fp:\n",
        "            json.dump(summary, fp, indent=4)\n",
        "\n",
        "        console.print(Panel(f\"[bold green]✓ Benchmarking Complete. Recall@{args.recall_k_val}: {recall:.2f}%[/]\", expand=False))\n",
        "\n",
        "        # Rich table output\n",
        "        table = Table(title=f\"Benchmark Summary (Recall@{args.recall_k_val}) - {args.model_dir}\")\n",
        "        table.add_column(\"Metric\", style=\"cyan\")\n",
        "        table.add_column(\"Value\", style=\"magenta\")\n",
        "        for key, value in summary.items():\n",
        "            if isinstance(value, float):\n",
        "                table.add_row(key, f\"{value:.4f}\")\n",
        "            else:\n",
        "                table.add_row(key, str(value))\n",
        "        console.print(table)\n",
        "        console.print(f\"[bold green]Summary saved to → {summary_save_path}[/]\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        console.print(f\"[bold red]Benchmarking Error: A required file was not found: {e}[/]\")\n",
        "    except ValueError as e:\n",
        "        console.print(f\"[bold red]Benchmarking Error: A value was incorrect: {e}[/]\")\n",
        "    except RuntimeError as e: # Catch CUDA errors, etc.\n",
        "        console.print(f\"[bold red]Benchmarking Runtime Error: {e}[/]\")\n",
        "        console.print(\"[bold yellow]This could be a CUDA OOM error. Try reducing batch size or number of samples.[/]\")\n",
        "    except Exception as e:\n",
        "        console.print(f\"[bold red]An unexpected error occurred during benchmarking: {e}[/]\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# -----------------------------\n",
        "# CLI\n",
        "# -----------------------------\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser(\n",
        "        description=\"LoRA + 8‑bit Retriever fine‑tuning & benchmarking (Robust Version)\",\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "    sub = p.add_subparsers(dest=\"command\", required=True, help=\"Specify 'train' or 'bench'\")\n",
        "\n",
        "    # Common args for model and data\n",
        "    p_common = argparse.ArgumentParser(add_help=False)\n",
        "    p_common.add_argument(\"--dataset_path\", type=str, required=True, help=\"Path to the root of the dataset (e.g., data/Amazon670K)\")\n",
        "    p_common.add_argument(\"--model_name\", type=str, default=\"sentence-transformers/all-mpnet-base-v2\", help=\"Base model name from Hugging Face Hub\")\n",
        "    p_common.add_argument(\"--max_seq_length\", type=int, default=128, help=\"Maximum sequence length for tokenizer\")\n",
        "    p_common.add_argument(\"--topk_labels_train\", type=int, default=None, help=\"Use only top K labels per sample for training (None for all)\")\n",
        "    p_common.add_argument(\"--topk_labels_val\", type=int, default=None, help=\"Use only top K labels per sample for validation/testing (None for all)\")\n",
        "\n",
        "\n",
        "    # TRAIN arguments\n",
        "    p_train = sub.add_parser(\"train\", parents=[p_common], help=\"Fine-tune a model\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    p_train.add_argument(\"--output_dir\", type=str, required=True, help=\"Directory to save trained model checkpoints and logs\")\n",
        "    p_train.add_argument(\"--epochs\", type=int, default=1, help=\"Number of training epochs\")\n",
        "    p_train.add_argument(\"--batch_size\", type=int, default=12, help=\"Training batch size per device\")\n",
        "    p_train.add_argument(\"--eval_batch_size\", type=int, default=32, help=\"Evaluation batch size per device\")\n",
        "    p_train.add_argument(\"--grad_accum\", type=int, default=2, help=\"Gradient accumulation steps\")\n",
        "    p_train.add_argument(\"--lr\", type=float, default=2e-5, help=\"Learning rate\")\n",
        "    p_train.add_argument(\"--weight_decay\", type=float, default=0.01, help=\"Weight decay for optimizer\")\n",
        "    p_train.add_argument(\"--lora_rank\", type=int, default=8, help=\"LoRA rank (r)\")\n",
        "    p_train.add_argument(\"--lora_alpha\", type=int, default=16, help=\"LoRA alpha\")\n",
        "    p_train.add_argument(\"--train_samples\", type=int, default=500_000, help=\"Max training samples to use (None for all)\")\n",
        "    p_train.add_argument(\"--val_samples\", type=int, default=5_000, help=\"Max validation samples to use (None for all)\")\n",
        "    p_train.add_argument(\"--logging_steps\", type=int, default=100, help=\"Log training metrics every N steps\")\n",
        "    p_train.add_argument(\"--save_total_limit\", type=int, default=1, help=\"Max number of checkpoints to keep\")\n",
        "    p_train.add_argument(\"--num_workers\", type=int, default=2, help=\"Number of dataloader workers\")\n",
        "    p_train.add_argument(\"--report_to\", type=str, default=\"none\", help=\"Reporting integrations (e.g. 'tensorboard', 'wandb', 'none')\")\n",
        "\n",
        "\n",
        "    # BENCH arguments\n",
        "    p_bench = sub.add_parser(\"bench\", parents=[p_common], help=\"Benchmark a trained model\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    p_bench.add_argument(\"--model_dir\", type=str, required=True, help=\"Directory of the fine-tuned LoRA model (adapters)\")\n",
        "    # Need to specify base model for bench too, as LoRA adapters are applied on top.\n",
        "    p_bench.add_argument(\"--base_model_name_for_bench\", type=str, default=\"sentence-transformers/all-mpnet-base-v2\", help=\"Base model name that was used for fine-tuning the adapters in --model_dir\")\n",
        "    p_bench.add_argument(\"--eval_batch_size\", type=int, default=64, help=\"Batch size for encoding corpus and queries\")\n",
        "    p_bench.add_argument(\"--val_samples\", type=int, default=10_000, help=\"Max query samples for benchmarking (from test set)\")\n",
        "    p_bench.add_argument(\"--index_samples\", type=int, default=100_000, help=\"Max corpus samples for FAISS index (from train set)\")\n",
        "    p_bench.add_argument(\"--recall_k_val\", type=int, default=100, help=\"Value of K for recall@K calculation\")\n",
        "\n",
        "    # Override model_name for bench as it's implicitly defined by base_model_name_for_bench and model_dir\n",
        "    # This avoids conflict if --model_name is also passed with bench command by mistake.\n",
        "    p_bench.set_defaults(model_name=None)\n",
        "\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache() # Clear cache at the very beginning\n",
        "\n",
        "\n",
        "    import sys\n",
        "\n",
        "    sys.argv = [\n",
        "        'my_script_name.py', # Placeholder for script name\n",
        "        'train',\n",
        "        '--dataset_path', '/content/drive/MyDrive/LORA',\n",
        "        '--output_dir', '/content/exp/mpnet_lora8_8bit_robust/',\n",
        "        '--epochs', '1',\n",
        "        '--model_name', 'sentence-transformers/all-mpnet-base-v2',\n",
        "        '--lora_rank', '8',\n",
        "        '--lora_alpha', '16'\n",
        "        # ... add other necessary arguments for train or bench\n",
        "    ]\n",
        "\n",
        "    args = parse_args()\n",
        "\n",
        "    console.print(Panel(f\"[bold bright_magenta]Executing Command: {args.command}[/]\", expand=False, border_style=\"bright_magenta\"))\n",
        "    console.print(f\"Arguments: {vars(args)}\")\n",
        "\n",
        "    if args.command == \"train\":\n",
        "        # For training, args.model_name is the base model to start from\n",
        "        if not args.model_name: # From p_common, should be set unless explicitly None\n",
        "             console.print(\"[bold red]Error: --model_name must be specified for training.[/]\")\n",
        "             sys.exit(1)\n",
        "        train(args)\n",
        "    elif args.command == \"bench\":\n",
        "        # For benchmarking, args.base_model_name_for_bench is the original base\n",
        "        # and args.model_dir contains the LoRA adapters.\n",
        "        if not args.base_model_name_for_bench:\n",
        "            console.print(\"[bold red]Error: --base_model_name_for_bench must be specified for benchmarking to load adapters correctly.[/]\")\n",
        "            sys.exit(1)\n",
        "        bench(args)\n",
        "    else:\n",
        "        console.print(f\"[bold red]Unknown command: {args.command}[/]\")\n",
        "        sys.exit(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "kI_YMQwBlf5q",
        "outputId": "293990e6-3a06-47a6-9f73-c5169ee0900a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[95m╭──────────────────────────╮\u001b[0m\n",
              "\u001b[95m│\u001b[0m \u001b[1;95mExecuting Command: train\u001b[0m \u001b[95m│\u001b[0m\n",
              "\u001b[95m╰──────────────────────────╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">╭──────────────────────────╮</span>\n",
              "<span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">│</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Executing Command: train</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">│</span>\n",
              "<span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">╰──────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Arguments: \u001b[1m{\u001b[0m\u001b[32m'command'\u001b[0m: \u001b[32m'train'\u001b[0m, \u001b[32m'dataset_path'\u001b[0m: \u001b[32m'/content/drive/MyDrive/LORA'\u001b[0m, \u001b[32m'model_name'\u001b[0m: \n",
              "\u001b[32m'sentence-transformers/all-mpnet-base-v2'\u001b[0m, \u001b[32m'max_seq_length'\u001b[0m: \u001b[1;36m128\u001b[0m, \u001b[32m'topk_labels_train'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'topk_labels_val'\u001b[0m: \n",
              "\u001b[3;35mNone\u001b[0m, \u001b[32m'output_dir'\u001b[0m: \u001b[32m'/content/exp/mpnet_lora8_8bit_robust/'\u001b[0m, \u001b[32m'epochs'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m12\u001b[0m, \u001b[32m'eval_batch_size'\u001b[0m: \u001b[1;36m32\u001b[0m, \n",
              "\u001b[32m'grad_accum'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'lr'\u001b[0m: \u001b[1;36m2e-05\u001b[0m, \u001b[32m'weight_decay'\u001b[0m: \u001b[1;36m0.01\u001b[0m, \u001b[32m'lora_rank'\u001b[0m: \u001b[1;36m8\u001b[0m, \u001b[32m'lora_alpha'\u001b[0m: \u001b[1;36m16\u001b[0m, \u001b[32m'train_samples'\u001b[0m: \u001b[1;36m500000\u001b[0m, \n",
              "\u001b[32m'val_samples'\u001b[0m: \u001b[1;36m5000\u001b[0m, \u001b[32m'logging_steps'\u001b[0m: \u001b[1;36m100\u001b[0m, \u001b[32m'save_total_limit'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'num_workers'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'report_to'\u001b[0m: \u001b[32m'none'\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Arguments: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'command'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'train'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'dataset_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/content/drive/MyDrive/LORA'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'sentence-transformers/all-mpnet-base-v2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'max_seq_length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'topk_labels_train'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'topk_labels_val'</span>: \n",
              "<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/content/exp/mpnet_lora8_8bit_robust/'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'epochs'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'eval_batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'grad_accum'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lr'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2e-05</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'weight_decay'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lora_rank'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'lora_alpha'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'train_samples'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">500000</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'val_samples'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5000</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'logging_steps'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'save_total_limit'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_workers'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'report_to'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'none'</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭───────────────────────────────╮\n",
              "│ \u001b[1;32mInitializing Training Process\u001b[0m │\n",
              "╰───────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭───────────────────────────────╮\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Initializing Training Process</span> │\n",
              "╰───────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33mFinal LoRA adapter model and tokenizer already found in \u001b[0m\u001b[1;33m/content/exp/mpnet_lora8_8bit_robust/\u001b[0m\u001b[1;33m.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Final LoRA adapter model and tokenizer already found in /content/exp/mpnet_lora8_8bit_robust/.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[33mSkipping fine-tuning. To re-train, clear or use a different --output_dir.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Skipping fine-tuning. To re-train, clear or use a different --output_dir.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/file.zip /content/exp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JQLGN_yWDWq",
        "outputId": "443bb12e-e138-41b2-ff71-517381e1e80c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/exp/ (stored 0%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/ (stored 0%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/training_args.bin (deflated 51%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/trainer_state.json (deflated 75%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/vocab.txt (deflated 53%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/checkpoint-5618/ (stored 0%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/checkpoint-5618/training_args.bin (deflated 51%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/checkpoint-5618/trainer_state.json (deflated 75%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/checkpoint-5618/adapter_model.safetensors (deflated 7%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/checkpoint-5618/adapter_config.json (deflated 50%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/checkpoint-5618/optimizer.pt (deflated 18%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/checkpoint-5618/rng_state.pth (deflated 25%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/checkpoint-5618/scheduler.pt (deflated 54%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/checkpoint-5618/README.md (deflated 66%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/adapter_model.safetensors (deflated 7%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/adapter_config.json (deflated 50%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/special_tokens_map.json (deflated 85%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/tokenizer_config.json (deflated 76%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/tokenizer.json (deflated 71%)\n",
            "updating: content/exp/mpnet_lora8_8bit_robust/README.md (deflated 66%)\n",
            "  adding: content/exp/mpnet_lora8_8bit_robust/bench_summary_k100.json (deflated 43%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bench Marking"
      ],
      "metadata": {
        "id": "xdH7qxfMX8Z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA + 8‑bit Fine‑Tuning for Extreme Multi‑Label Retrieval (Redesigned for Robustness)\n",
        "# Author: Shivaram Kumar (Systems Researcher)\n",
        "# Original Time‑frame: Jan – Feb 2025\n",
        "# Redesign: For error handling and Colab-friendliness\n",
        "# Hardware target: single RTX 3060 Max‑Q (original), Google Colab T4 GPU\n",
        "#\n",
        "# -----------------------------------------------------------------------------\n",
        "# This script covers the full reproducible pipeline:\n",
        "#   • parameter‑efficient fine‑tuning of an MPNet retriever with LoRA (r=8)\n",
        "#   • 8‑bit Adam optimiser via bitsandbytes\n",
        "#   • recall@100 evaluation for extreme multi‑label retrieval (Amazon‑670K‑like)\n",
        "#   • timing / throughput profiler\n",
        "#   • automatic logging of cost–speed trade‑offs\n",
        "#\n",
        "# The code is organised as *one* Python file that can be executed in two modes:\n",
        "#   1.  TRAIN   – fine‑tune and save the LoRA‑adapted 8‑bit model + tokenizer\n",
        "#   2.  BENCH   – measure GPU memory, wall‑clock throughput and recall@100\n",
        "#\n",
        "# Example usage (after placing dataset in data/Amazon670K):\n",
        "#   $ python lora_8bit_retrieval_robust.py train \\\n",
        "#       --dataset_path data/Amazon670K \\\n",
        "#       --output_dir exp/mpnet_lora8_8bit_robust\n",
        "#   $ python lora_8bit_retrieval_robust.py bench \\\n",
        "#       --dataset_path data/Amazon670K \\\n",
        "#       --model_dir exp/mpnet_lora8_8bit_robust\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "#\n",
        "# === FOR GOOGLE COLAB: Run this cell first ===\n",
        "#\n",
        "# !pip install torch==2.2.* torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install transformers==4.41.*\n",
        "# !pip install peft==0.11.*\n",
        "# !pip install bitsandbytes==0.43.*\n",
        "# !pip install datasets==2.19.*\n",
        "# !pip install faiss-cpu==1.8.* # or faiss-gpu if you prefer and have VRAM\n",
        "# !pip install accelerate==0.28.*\n",
        "# !pip install rich>=13\n",
        "# !pip install sentence-transformers # For all-mpnet-base-v2, though AutoModel handles it\n",
        "#\n",
        "# ========================================\n",
        "#\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache() # Clear cache at the very beginning\n",
        "\n",
        "\n",
        "    import sys\n",
        "\n",
        "    sys.argv = [\n",
        "        'my_script_name.py',        # Placeholder for script name\n",
        "        'bench',                    # <--- CHANGE TO 'bench'\n",
        "        '--dataset_path', '/content/drive/MyDrive/LORA',\n",
        "        '--model_dir', '/content/exp/mpnet_lora8_8bit_robust/', # Dir where trained LoRA is\n",
        "        '--base_model_name_for_bench', 'sentence-transformers/all-mpnet-base-v2', # Original base\n",
        "        '--recall_k_val', '100',    # Example, default is 100\n",
        "        # Add other bench-specific args if needed, e.g.:\n",
        "        # '--val_samples', '1000',\n",
        "        # '--index_samples', '10000'\n",
        "    ]\n",
        "    args = parse_args() # This will now parse bench arguments\n",
        "    # ... the rest of your if/elif block will now execute bench(args)\n",
        "\n",
        "    args = parse_args()\n",
        "\n",
        "    console.print(Panel(f\"[bold bright_magenta]Executing Command: {args.command}[/]\", expand=False, border_style=\"bright_magenta\"))\n",
        "    console.print(f\"Arguments: {vars(args)}\")\n",
        "\n",
        "    if args.command == \"train\":\n",
        "        # For training, args.model_name is the base model to start from\n",
        "        if not args.model_name: # From p_common, should be set unless explicitly None\n",
        "             console.print(\"[bold red]Error: --model_name must be specified for training.[/]\")\n",
        "             sys.exit(1)\n",
        "        train(args)\n",
        "    elif args.command == \"bench\":\n",
        "        # For benchmarking, args.base_model_name_for_bench is the original base\n",
        "        # and args.model_dir contains the LoRA adapters.\n",
        "        if not args.base_model_name_for_bench:\n",
        "            console.print(\"[bold red]Error: --base_model_name_for_bench must be specified for benchmarking to load adapters correctly.[/]\")\n",
        "            sys.exit(1)\n",
        "        bench(args)\n",
        "    else:\n",
        "        console.print(f\"[bold red]Unknown command: {args.command}[/]\")\n",
        "        sys.exit(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "oV_bEYZSX70D",
        "outputId": "d82b2fda-5ccb-4f73-d6e6-46cc7d1f714d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[95m╭──────────────────────────╮\u001b[0m\n",
              "\u001b[95m│\u001b[0m \u001b[1;95mExecuting Command: bench\u001b[0m \u001b[95m│\u001b[0m\n",
              "\u001b[95m╰──────────────────────────╯\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">╭──────────────────────────╮</span>\n",
              "<span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">│</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Executing Command: bench</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">│</span>\n",
              "<span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">╰──────────────────────────╯</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Arguments: \u001b[1m{\u001b[0m\u001b[32m'command'\u001b[0m: \u001b[32m'bench'\u001b[0m, \u001b[32m'dataset_path'\u001b[0m: \u001b[32m'/content/drive/MyDrive/LORA'\u001b[0m, \u001b[32m'model_name'\u001b[0m: \u001b[3;35mNone\u001b[0m, \n",
              "\u001b[32m'max_seq_length'\u001b[0m: \u001b[1;36m128\u001b[0m, \u001b[32m'topk_labels_train'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'topk_labels_val'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'model_dir'\u001b[0m: \n",
              "\u001b[32m'/content/exp/mpnet_lora8_8bit_robust/'\u001b[0m, \u001b[32m'base_model_name_for_bench'\u001b[0m: \u001b[32m'sentence-transformers/all-mpnet-base-v2'\u001b[0m, \n",
              "\u001b[32m'eval_batch_size'\u001b[0m: \u001b[1;36m64\u001b[0m, \u001b[32m'val_samples'\u001b[0m: \u001b[1;36m10000\u001b[0m, \u001b[32m'index_samples'\u001b[0m: \u001b[1;36m100000\u001b[0m, \u001b[32m'recall_k_val'\u001b[0m: \u001b[1;36m100\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Arguments: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'command'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'bench'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'dataset_path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/content/drive/MyDrive/LORA'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'max_seq_length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'topk_labels_train'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'topk_labels_val'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model_dir'</span>: \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'/content/exp/mpnet_lora8_8bit_robust/'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'base_model_name_for_bench'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'sentence-transformers/all-mpnet-base-v2'</span>, \n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'eval_batch_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'val_samples'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'index_samples'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100000</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'recall_k_val'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36mLoading sentence-transformers/all-mpnet-base-v2 \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;36m‑\u001b[0m\u001b[1;36mbit\u001b[0m\u001b[1;36m=\u001b[0m\u001b[1;3;36mTrue\u001b[0m\u001b[1;36m)\u001b[0m\u001b[1;36m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Loading sentence-transformers/all-mpnet-base-v2 (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">‑</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">bit</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">True</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[33mWarning: MPNet models have issues with \u001b[0m\u001b[1;33m8\u001b[0m\u001b[33m-bit quantization. Loading with fp16 instead.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000\">Warning: MPNet models have issues with </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">8</span><span style=\"color: #808000; text-decoration-color: #808000\">-bit quantization. Loading with fp16 instead.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[32m✓ Model sentence-transformers/all-mpnet-base-v2 loaded successfully \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mquantized: \u001b[0m\u001b[3;32mFalse\u001b[0m\u001b[1;32m)\u001b[0m\u001b[32m.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Model sentence-transformers/all-mpnet-base-v2 loaded successfully </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">quantized: </span><span style=\"color: #008000; text-decoration-color: #008000; font-style: italic\">False</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">)</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[32m✓ Fine-tuned LoRA model and tokenizer loaded from \u001b[0m\u001b[32m/content/exp/mpnet_lora8_8bit_robust/\u001b[0m\u001b[32m.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ Fine-tuned LoRA model and tokenizer loaded from /content/exp/mpnet_lora8_8bit_robust/.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[36mLoading corpus \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mfrom training set\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m and query \u001b[0m\u001b[1;36m(\u001b[0m\u001b[36mfrom test set\u001b[0m\u001b[1;36m)\u001b[0m\u001b[36m data\u001b[0m\u001b[36m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Loading corpus </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">from training set</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\"> and query </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080\">from test set</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #008080; text-decoration-color: #008080\"> data...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[36mEncoding corpus items\u001b[0m\u001b[36m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Encoding corpus items...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Corpus encoding took: \u001b[1;36m72.\u001b[0m12s for \u001b[1;36m100000\u001b[0m items.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Corpus encoding took: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">72.</span>12s for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100000</span> items.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[32m✓ FAISS index built with \u001b[0m\u001b[1;32m100000\u001b[0m\u001b[32m embeddings of dim \u001b[0m\u001b[1;32m768\u001b[0m\u001b[32m.\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✓ FAISS index built with </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">100000</span><span style=\"color: #008000; text-decoration-color: #008000\"> embeddings of dim </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">768</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  FAISS index build took: \u001b[1;36m0.\u001b[0m86s.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  FAISS index build took: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>86s.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[36mEncoding query items\u001b[0m\u001b[36m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Encoding query items...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Query encoding took: \u001b[1;36m6.\u001b[0m67s for \u001b[1;36m10000\u001b[0m items.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  Query encoding took: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.</span>67s for <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000</span> items.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[36mPerforming search and calculating recall@\u001b[0m\u001b[1;36m100\u001b[0m\u001b[36m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">Performing search and calculating recall@</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #008080; text-decoration-color: #008080\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  FAISS search and recall calculation took: \u001b[1;36m25.\u001b[0m49s.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">  FAISS search and recall calculation took: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.</span>49s.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭──────────────────────────────────────────────╮\n",
              "│ \u001b[1;32m✓ Benchmarking Complete. Recall@100: 100.00%\u001b[0m │\n",
              "╰──────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────────────╮\n",
              "│ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">✓ Benchmarking Complete. Recall@100: 100.00%</span> │\n",
              "╰──────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m            Benchmark Summary (Recall@100) - /content/exp/mpnet_lora8_8bit_robust/             \u001b[0m\n",
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mMetric                                           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mValue                                  \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36mtimestamp                                        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m2025-05-29T23:37:34                    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mmodel_dir                                        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m/content/exp/mpnet_lora8_8bit_robust/  \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mbase_model_name                                  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35msentence-transformers/all-mpnet-base-v2\u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mdataset_path                                     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m/content/drive/MyDrive/LORA            \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mindex_samples                                    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m100000                                 \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mquery_samples                                    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m10000                                  \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mrecall@100                                       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m100.0000                               \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mthroughput_qps (queries_encoded_and_searched/sec)\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m311.0056                               \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mcorpus_encode_time_s                             \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m72.1236                                \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mfaiss_index_build_time_s                         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.8639                                 \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mtotal_index_build_time_s                         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m72.9875                                \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mquery_encode_time_s                              \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m6.6685                                 \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36msearch_and_recall_calc_time_s                    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m25.4853                                \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36mgpu_peak_mem_gb                                  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.9305                                 \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────────────────────────────┴─────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">            Benchmark Summary (Recall@100) - /content/exp/mpnet_lora8_8bit_robust/             </span>\n",
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Metric                                            </span>┃<span style=\"font-weight: bold\"> Value                                   </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> timestamp                                         </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 2025-05-29T23:37:34                     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> model_dir                                         </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> /content/exp/mpnet_lora8_8bit_robust/   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> base_model_name                                   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> sentence-transformers/all-mpnet-base-v2 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> dataset_path                                      </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> /content/drive/MyDrive/LORA             </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> index_samples                                     </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 100000                                  </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> query_samples                                     </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 10000                                   </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> recall@100                                        </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 100.0000                                </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> throughput_qps (queries_encoded_and_searched/sec) </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 311.0056                                </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> corpus_encode_time_s                              </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 72.1236                                 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> faiss_index_build_time_s                          </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 0.8639                                  </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> total_index_build_time_s                          </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 72.9875                                 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> query_encode_time_s                               </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 6.6685                                  </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> search_and_recall_calc_time_s                     </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 25.4853                                 </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\"> gpu_peak_mem_gb                                   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 0.9305                                  </span>│\n",
              "└───────────────────────────────────────────────────┴─────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mSummary saved to → \u001b[0m\u001b[1;32m/content/exp/mpnet_lora8_8bit_robust/\u001b[0m\u001b[1;32mbench_summary_k100.json\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Summary saved to → /content/exp/mpnet_lora8_8bit_robust/bench_summary_k100.json</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}